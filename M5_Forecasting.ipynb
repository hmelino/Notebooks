{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#M5 Forecasting\n",
    "\n",
    "Kaggle's competition M5 Forecasting - Accuracy dataset consist of Walmart's sales dataset across their shops in 3 states of America. In this notebook we will try to predict sales for 28 days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Model\n",
    "\n",
    "Facebook's Prophet will be model of our choice.\n",
    "- easy to use without expert knowledge on statistics or time series forecasting\n",
    "- allows to use multiple seasonabilities \n",
    "- easy to input holiday effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Objectives\n",
    "- inspect and analyse provided datasets\n",
    "- extract holiday from calendar and use it within our model\n",
    "- train our model and predict future 28 days of sales for each item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "from fbprophet import Prophet\n",
    "from fbprophet.diagnostics import cross_validation , performance_metrics\n",
    "from fbprophet.plot import plot_cross_validation_metric\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pd.options.display.float_format = '{:20,.2f}'.format "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By assigning dtypes we can reduce RAM usage for each dataframe.\n",
    "- `sell_prices` was reduced from 0.94GB to 0.86GB\n",
    "- `sales_train_val` was reduced from 0.45GB to 0.07GB\n",
    "- `calendar` dataframe size is only 0.09MB , so we left it untouched\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "calendar=pd.read_csv('/Users/hmelino/Desktop/Coding/m5-forecasting-accuracy/calendar.csv')\n",
    "sales_train_val=pd.read_csv('/Users/hmelino/Desktop/Coding/m5-forecasting-accuracy/sales_train_validation.csv',dtype={f'd_{v}':'int8' for v in range(1,1914)})\n",
    "#sample_sub=pd.read_csv('/content/drive/My Drive/Reports/m5-forecasting-accuracy/sample_submission.csv')[:30490]\n",
    "sell_prices=pd.read_csv('/Users/hmelino/Desktop/Coding/m5-forecasting-accuracy/sell_prices.csv',dtype={'wm_yr_wk':'int16','sell_price':'float16'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Exploring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_train_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sales_train_val is the main dataset we will use. Each row contains `item_id` , deparment, category , `store_id` , `state_id` and over 5 years of sales ( 1913 days ). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Holiday\n",
    "\n",
    "Holiday feature helps us capture seasonability of dataset. With use of `calendar` dataset , we will :\n",
    "\n",
    "- translate '**wm_yr_wk**' value `11101` into `2011-01-29` for our **sell_price** dataset\n",
    "\n",
    "- create **holiday** dataset for our Prophet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using dictionary comprehension we will extract and order all events with coresponding dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_dict = {holidayName:[date for date in calendar.loc[calendar['event_name_1']==holidayName]['date'].values] for holidayName in calendar['event_name_1'].unique()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_list=[pd.DataFrame({'holiday':hol_name,'ds':pd.to_datetime(dates)}) for hol_name,dates in holiday_dict.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_list[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All converted dataframes will be now atatched to main holiday dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "holiday = pd.DataFrame(columns=['holiday','ds'])\n",
    "for d in holiday_list:\n",
    "  holiday = pd.concat([holiday,d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#release RAM and remove unececssary variables\n",
    "del holiday_list\n",
    "del holiday_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##X_train transfomation\n",
    "\n",
    "`sales_train_val` contains all training values for our model.\n",
    "\n",
    "One of the best ways to start data transforming is to remove unecessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df=sales_train_val.drop(['item_id','dept_id','cat_id','store_id','state_id'],axis=1).set_index(sales_train_val['id'].str[:-11]).T[1:].set_index(calendar['date'][:1913])\n",
    "\n",
    "cat_list = list(df.columns)\n",
    "future = pd.DataFrame({'ds':pd.date_range(start='2016-04-25',end='2016-05-23',freq='d')})\n",
    "column = 'HOBBIES_1_001_CA_1'\n",
    "\n",
    "#free up memory and remove unecessary variable\n",
    "del sales_train_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Facebook Prophet will include training data in predictions.\n",
    "\n",
    "However with such a big dataset, every second of computing time is precious. To speed up such a time consuming task we will specify 28 days we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future = pd.DataFrame({'ds':pd.date_range(start='2016-04-25',end='2016-05-26')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Fit & Predict\n",
    "\n",
    "By specifying parameters, our model will be more accurate and faster.\n",
    "\n",
    "- `daily_seasonality=False`,`weekly_seasonality=False`,`yearly_seasonality=True` - specify seasonability\n",
    "- `uncertainity_samples=0` as we dont need values for yhat_lower and yhat_upper\n",
    "- `holidays` - seasonability, ability to catch extra high and extra low selling days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "m = Prophet(daily_seasonality=False,weekly_seasonality=False,yearly_seasonality=True,uncertainty_samples=0,holidays=holiday)\n",
    "data = pd.DataFrame({'ds':df[column].index,'y':df[column].values})\n",
    "m.fit(data)\n",
    "predicted_values = m.predict(future)['yhat'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have trained and predicted future 28 days of sales for column `HOBBIES_1_001_CA_1`.\n",
    "But how good are our predicitons ? Cross validation is a good tool to find out accuracy of our model.\n",
    "\n",
    "Luckily , Prophet comes already with cross validation tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_cross_val = cross_validation(m,initial='1095 days',period='365 days',horizon='20 days' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cross_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "performance_metrics(df_cross_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cross_validation_metric(df_cross_val,metric='mae')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is doing pretty well, as `MAE` - mean absolute error is withing appropriate range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Fit & Predict 30490 row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def run_prophet(column):\n",
    "  m = Prophet(daily_seasonality=False,weekly_seasonality=False,yearly_seasonality=True,uncertainty_samples=0,holidays=holiday)\n",
    "  data = pd.DataFrame({'ds':df[column].index,'y':df[column].values})\n",
    "  m.fit(data)\n",
    "  return m.predict(future)['yhat'].values\n",
    "\n",
    "run_prophet(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our function that will predict 28 days of sales for given column/item. \n",
    "\n",
    "Cell below will use this function to predict 30490 columns/items. On Google's Cloud with 16 cpu's it took just under an hour to complete. \n",
    "\n",
    "To avoid running this cell every single time we run this notebook , I have saved output into pickle file , and commented out whole cell, to prevent it from running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from multiprocessing import Pool,cpu_count\n",
    "from tqdm import tqdm\n",
    "p = Pool(cpu_count())\n",
    "predictions = list(tqdm(p.imap(run_prophet,cat_list),total=len(cat_list)))\n",
    "p.close()\n",
    "p.join()\n",
    "\n",
    "import pickle\n",
    "with open('new_prediction.pickle','wb') as pickle_file:\n",
    "  pickle.dump(predictions,pickle_file)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\[OUT]\n",
    "100%|██████████| 30490/30490 \\[58:32<00:00,  8.68it/s] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets load our saved predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=pickle.load(open('/Users/hmelino/Desktop/Coding/m5-forecasting-accuracy/new_prediction.pickle','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To double check that we have all predicted values, we will use len() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is going according to plan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Organise forecast data\n",
    "\n",
    "As we know , we are still dealing with a large size of data and while designing the fastest function, I have learned a lot about how Pandas and Python work with data. \n",
    "\n",
    "##My first attempt:\n",
    "\n",
    "[IN]\n",
    "```\n",
    "%%time\n",
    "c_names = [f'F{n}' for n in range(1,29)]\n",
    "final_df = pd.DataFrame(columns=['id']+c_names)\n",
    "for i in range(len(future_sales)):\n",
    "  new_row_df = pd.DataFrame(future_sales[i],index=c_names).T\n",
    "  new_row_df['id']=df.columns[i]\n",
    "  final_df=pd.concat([final_df,new_row_df])\n",
    "```\n",
    "[OUT]\n",
    "\n",
    "\n",
    "```\n",
    "CPU times: user 1min 50s, sys: 1.06 s, total: 1min 51s\n",
    "Wall time: 1min 52s\n",
    "```\n",
    "\n",
    "... and improved version :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "c_names = [f'F{n}' for n in range(1,29)]\n",
    "\n",
    "forecast = pd.concat([pd.DataFrame(predictions[i][:28],index=c_names).T for i in tqdm(range(len(predictions)))])\n",
    "\n",
    "forecast['id']=[v for v in df.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see , there is always a room for improvement , our new function is 7x faster.\n",
    "\n",
    "We have applied these improvements:\n",
    "- using list comprehension\n",
    "\n",
    "- concatenating all dataframes together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have organised forecast data, we can attach them to main dataset and visualise our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[25,10])\n",
    "states = {'CA':'California','WI':'Wisconsin','TX':'Texas'}\n",
    "\n",
    "for c in range(1,len(states.keys())+1):\n",
    "  plt.subplot(3,1,c) #position of plot\n",
    "  plt.xlim(1050,1950) #cut first 1050 days as there were no sales\n",
    "  state_code = list(states.keys())[c-1] #get state code from states dict\n",
    "\n",
    "  plt.title(states[state_code]) #set title of plot to state name\n",
    "  plot_df = pd.DataFrame({'y': list(df[f'HOBBIES_1_354_{state_code}_1'].values.astype('float')) + list(forecast.loc[forecast['id']==f'HOBBIES_1_008_{state_code}_1'][[c for c in forecast.columns if 'F' in c]].values[0])  })\n",
    "  sns.lineplot(data=plot_df,x=plot_df.index,y='y')\n",
    "  plt.gca().set(xticklabels = [pd.date_range(start = '2011-01-29', end= '2016-05-26')[int(1941/11*v)].strftime('%Y-%m-%d') for v in range(10)])\n",
    "\n",
    "  plt.axvspan(1912,1940,alpha=0.3,label='prediction',color='red')\n",
    "  plt.tight_layout(pad=3.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have sucesfully predicted 28 days of sales for each item in our dataset. \n",
    "\n",
    "Analysis comes in separate notebook very soon ...  \n",
    "\n",
    "Thank you for your attention :)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bitbaseconda468fdac5fcbe4a0cac556d7f81e0e822",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}